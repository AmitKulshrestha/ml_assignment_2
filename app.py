# -*- coding: utf-8 -*-
"""app.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1cxq9JZyLpKb1rIuMSYxpduWZP14C4XY7
"""

import streamlit as st
import pandas as pd
import numpy as np
import joblib
import plotly.express as px
import plotly.graph_objects as go
from sklearn.metrics import (accuracy_score, roc_auc_score, precision_score,
                           recall_score, f1_score, matthews_corrcoef,
                           confusion_matrix, classification_report)
import seaborn as sns
import matplotlib.pyplot as plt
import os
import warnings
warnings.filterwarnings('ignore')

# Page configuration
st.set_page_config(
    page_title="ML Classification Models - Assignment 2",
    page_icon="ü§ñ",
    layout="wide",
    initial_sidebar_state="expanded"
)

# Custom CSS
st.markdown("""
<style>
    .main-header {
        font-size: 2.5rem;
        color: #1E88E5;
        text-align: center;
        padding: 1rem;
        background: linear-gradient(90deg, #f8f9fa 0%, #e9ecef 100%);
        border-radius: 10px;
        margin-bottom: 2rem;
    }
    .sub-header {
        font-size: 1.5rem;
        color: #0d47a1;
        padding: 0.5rem;
        border-left: 5px solid #1E88E5;
        background-color: #f0f2f6;
        border-radius: 0 10px 10px 0;
        margin: 1.5rem 0;
    }
    .metric-card {
        background-color: white;
        padding: 1rem;
        border-radius: 10px;
        box-shadow: 0 4px 6px rgba(0,0,0,0.1);
        border: 1px solid #e0e0e0;
    }
    .stButton>button {
        width: 100%;
        background-color: #1E88E5;
        color: white;
        font-weight: bold;
    }
    .footer {
        text-align: center;
        padding: 2rem;
        color: #666;
        font-size: 0.8rem;
    }
</style>
""", unsafe_allow_html=True)

# Header
st.markdown('<h1 class="main-header">ü§ñ Machine Learning Classification Models</h1>', unsafe_allow_html=True)
st.markdown('<p style="text-align: center; font-size: 1.2rem; color: #666;">M.Tech (AIML/DSE) - Assignment 2 | Multi-Model Classification Comparison</p>', unsafe_allow_html=True)

# Initialize session state
if 'model_comparison' not in st.session_state:
    st.session_state.model_comparison = None
if 'selected_model' not in st.session_state:
    st.session_state.selected_model = None

# Load pre-trained models and scaler
@st.cache_resource
def load_models_and_scaler():
    """Load models with proper path resolution"""
    models = {}
    model_names = ['logistic_regression', 'decision_tree', 'k_nearest_neighbor', 
                   'naive_bayes', 'random_forest', 'xgboost']
    
    try:
        # Get the absolute path to the models directory
        import os
        import sys
        
        # This works both locally and on Streamlit Cloud
        current_dir = os.path.dirname(os.path.abspath(__file__))
        models_dir = os.path.join(current_dir, 'models')
        
        # Debug: Print paths to see what's happening
        print(f"Current directory: {current_dir}")
        print(f"Models directory: {models_dir}")
        print(f"Models directory exists: {os.path.exists(models_dir)}")
        
        if not os.path.exists(models_dir):
            st.error(f"‚ùå Models directory not found at: {models_dir}")
            return None, None
        
        # List files in models directory
        files = os.listdir(models_dir)
        print(f"Files in models directory: {files}")
        
        # Load scaler
        scaler_path = os.path.join(models_dir, 'scaler.pkl')
        if os.path.exists(scaler_path):
            scaler = joblib.load(scaler_path)
            print("‚úÖ Scaler loaded successfully")
        else:
            st.error(f"‚ùå Scaler not found at: {scaler_path}")
            return None, None
        
        # Load models
        for name in model_names:
            model_path = os.path.join(models_dir, f'{name}.pkl')
            if os.path.exists(model_path):
                models[name] = joblib.load(model_path)
                print(f"‚úÖ Loaded {name}")
            else:
                st.error(f"‚ùå Model {name} not found at: {model_path}")
                return None, None
        
        return models, scaler
        
    except Exception as e:
        st.error(f"‚ùå Error loading models: {str(e)}")
        return None, None

# Load model comparison results
@st.cache_data
def load_comparison_results():
    try:
        comparison_df = pd.read_csv('models/model_comparison.csv', index_col=0)
        return comparison_df
    except:
        return None

# Sidebar
with st.sidebar:
    st.image("https://streamlit.io/images/brand/streamlit-logo-secondary-colormark-darktext.png", width=200)
    st.markdown("## üìä Model Configuration")

    # Dataset upload section
    st.markdown("### üìÅ Dataset Upload")
    st.markdown("*Upload test data (CSV format)*")
    uploaded_file = st.file_uploader("Choose a CSV file", type="csv", key="uploader")

    # Model selection
    st.markdown("### ü§ñ Model Selection")
    model_options = {
        'Logistic Regression': 'logistic_regression',
        'Decision Tree': 'decision_tree',
        'K-Nearest Neighbor': 'k_nearest_neighbor',
        'Naive Bayes': 'naive_bayes',
        'Random Forest': 'random_forest',
        'XGBoost': 'xgboost'
    }

    selected_model_name = st.selectbox(
        "Choose a classification model:",
        list(model_options.keys())
    )
    selected_model_key = model_options[selected_model_name]

    # Advanced options
    st.markdown("### ‚öôÔ∏è Advanced Options")
    show_feature_importance = st.checkbox("Show Feature Importance", value=True)
    show_cv_scores = st.checkbox("Show Cross-Validation Scores", value=True)

    # About section
    st.markdown("---")
    st.markdown("### ‚ÑπÔ∏è About")
    st.info("""
    **Dataset**: Heart Disease UCI
    - Features: 14
    - Instances: 1025
    - Task: Binary Classification

    **BITS Virtual Lab** - Assignment 2
    Submission Date: 15-Feb-2026
    """)

# Main content
col1, col2 = st.columns([2, 1])

with col1:
    st.markdown('<h2 class="sub-header">üìä Model Performance Comparison</h2>', unsafe_allow_html=True)

    # Load and display comparison table
    comparison_df = load_comparison_results()

    if comparison_df is not None:
        st.markdown("#### All Models - Evaluation Metrics")

        # Format the dataframe
        styled_df = comparison_df.style.format({
            'Accuracy': '{:.4f}',
            'AUC': '{:.4f}',
            'Precision': '{:.4f}',
            'Recall': '{:.4f}',
            'F1': '{:.4f}',
            'MCC': '{:.4f}'
        }).background_gradient(cmap='Blues', axis=None)

        st.dataframe(styled_df, use_container_width=True)
        st.session_state.model_comparison = comparison_df

        # Highlight best model
        st.markdown("#### üèÜ Best Performing Model")
        best_model = comparison_df['Accuracy'].idxmax()
        best_accuracy = comparison_df.loc[best_model, 'Accuracy']
        st.success(f"**{best_model}** achieved the highest accuracy: **{best_accuracy:.4f}**")
    else:
        st.warning("‚ö†Ô∏è Model comparison results not found. Please run train_models.py first.")

with col2:
    st.markdown('<h2 class="sub-header">üìà Quick Stats</h2>', unsafe_allow_html=True)

    if comparison_df is not None:
        # Display metrics in cards
        avg_accuracy = comparison_df['Accuracy'].mean()
        avg_auc = comparison_df['AUC'].mean()
        avg_f1 = comparison_df['F1'].mean()

        col2_1, col2_2 = st.columns(2)
        with col2_1:
            st.markdown('<div class="metric-card">', unsafe_allow_html=True)
            st.metric("Average Accuracy", f"{avg_accuracy:.4f}")
            st.markdown('</div>', unsafe_allow_html=True)

            st.markdown('<div class="metric-card">', unsafe_allow_html=True)
            st.metric("Average AUC", f"{avg_auc:.4f}")
            st.markdown('</div>', unsafe_allow_html=True)

        with col2_2:
            st.markdown('<div class="metric-card">', unsafe_allow_html=True)
            st.metric("Average F1 Score", f"{avg_f1:.4f}")
            st.markdown('</div>', unsafe_allow_html=True)

            st.markdown('<div class="metric-card">', unsafe_allow_html=True)
            st.metric("Total Models", "6")
            st.markdown('</div>', unsafe_allow_html=True)

# Model performance visualization
st.markdown('<h2 class="sub-header">üìä Performance Visualization</h2>', unsafe_allow_html=True)

if comparison_df is not None:
    col3, col4 = st.columns(2)

    with col3:
        # Radar chart for model comparison
        fig = go.Figure()

        for model in comparison_df.index:
            fig.add_trace(go.Scatterpolar(
                r=comparison_df.loc[model, ['Accuracy', 'AUC', 'Precision', 'Recall', 'F1', 'MCC']].values,
                theta=['Accuracy', 'AUC', 'Precision', 'Recall', 'F1', 'MCC'],
                fill='toself',
                name=model
            ))

        fig.update_layout(
            polar=dict(
                radialaxis=dict(
                    visible=True,
                    range=[0, 1]
                )),
            showlegend=True,
            title="Model Performance Radar Chart",
            height=500
        )

        st.plotly_chart(fig, use_container_width=True)

    with col4:
        # Bar chart comparison
        fig = go.Figure(data=[
            go.Bar(name='Accuracy', x=comparison_df.index, y=comparison_df['Accuracy']),
            go.Bar(name='F1 Score', x=comparison_df.index, y=comparison_df['F1']),
            go.Bar(name='AUC', x=comparison_df.index, y=comparison_df['AUC'])
        ])

        fig.update_layout(
            title="Key Metrics Comparison",
            xaxis_title="Models",
            yaxis_title="Score",
            barmode='group',
            height=500
        )

        st.plotly_chart(fig, use_container_width=True)

# Selected model analysis
st.markdown(f'<h2 class="sub-header">üîç Detailed Analysis: {selected_model_name}</h2>', unsafe_allow_html=True)

# Handle dataset upload and predictions
if uploaded_file is not None:
    try:
        test_data = pd.read_csv(uploaded_file)
        st.success(f"‚úÖ Dataset loaded successfully! Shape: {test_data.shape}")

        # Load models
        models, scaler = load_models_and_scaler()

        if models is not None and scaler is not None:
            # Prepare data
            if 'target' in test_data.columns:
                X_test = test_data.drop('target', axis=1)
                y_test = test_data['target']
            else:
                X_test = test_data
                y_test = None

            # Scale if necessary
            if selected_model_key in ['logistic_regression', 'k_nearest_neighbor']:
                X_test_scaled = scaler.transform(X_test)
                X_test_use = X_test_scaled
            else:
                X_test_use = X_test

            # Make predictions
            model = models[selected_model_key]
            y_pred = model.predict(X_test_use)

            # Display results in columns
            col5, col6 = st.columns(2)

            with col5:
                st.markdown("#### üìã Evaluation Metrics")

                if y_test is not None:
                    # Calculate metrics
                    accuracy = accuracy_score(y_test, y_pred)
                    if hasattr(model, "predict_proba"):
                        y_pred_proba = model.predict_proba(X_test_use)[:, 1]
                        auc = roc_auc_score(y_test, y_pred_proba)
                    else:
                        auc = roc_auc_score(y_test, y_pred)
                    precision = precision_score(y_test, y_pred)
                    recall = recall_score(y_test, y_pred)
                    f1 = f1_score(y_test, y_pred)
                    mcc = matthews_corrcoef(y_test, y_pred)

                    # Display metrics
                    metrics_data = {
                        'Metric': ['Accuracy', 'AUC', 'Precision', 'Recall', 'F1 Score', 'MCC'],
                        'Value': [accuracy, auc, precision, recall, f1, mcc]
                    }
                    metrics_df = pd.DataFrame(metrics_data)
                    st.dataframe(metrics_df.style.format({'Value': '{:.4f}'}), use_container_width=True)
                else:
                    st.info("‚ÑπÔ∏è Upload test data with 'target' column to see evaluation metrics")
                    st.dataframe(pd.DataFrame(y_pred, columns=['Predictions']).head(10))

            with col6:
                st.markdown("#### üî• Confusion Matrix")

                if y_test is not None:
                    cm = confusion_matrix(y_test, y_pred)

                    fig, ax = plt.subplots(figsize=(8, 6))
                    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=ax)
                    ax.set_xlabel('Predicted')
                    ax.set_ylabel('Actual')
                    ax.set_title(f'Confusion Matrix - {selected_model_name}')
                    st.pyplot(fig)

                    # Classification report
                    st.markdown("#### üìä Classification Report")
                    report = classification_report(y_test, y_pred, output_dict=True)
                    report_df = pd.DataFrame(report).transpose()
                    st.dataframe(report_df.style.format('{:.4f}'), use_container_width=True)
                else:
                    st.warning("‚ö†Ô∏è Cannot generate confusion matrix without true labels")

    except Exception as e:
        st.error(f"Error processing file: {str(e)}")
else:
    st.info("üëÜ Please upload a CSV file from the sidebar to test the model")

# Feature Importance (for applicable models)
if show_feature_importance and selected_model_key in ['decision_tree', 'random_forest', 'xgboost']:
    st.markdown("#### üåü Feature Importance")

    models, _ = load_models_and_scaler()
    if models is not None:
        model = models[selected_model_key]

        if hasattr(model, 'feature_importances_'):
            # Load feature names from training data
            df_sample = pd.read_csv("https://raw.githubusercontent.com/archd3sai/Datasets/main/heart.csv")
            X_train_full = df_sample.drop('target', axis=1)
            feature_names = X_train_full.columns

            importances = model.feature_importances_
            indices = np.argsort(importances)[::-1]

            # Create dataframe for visualization
            importance_df = pd.DataFrame({
                'Feature': [feature_names[i] for i in indices],
                'Importance': [importances[i] for i in indices]
            })

            # Plot
            fig = px.bar(importance_df.head(10),
                        x='Importance',
                        y='Feature',
                        orientation='h',
                        title=f'Top 10 Feature Importances - {selected_model_name}',
                        color='Importance',
                        color_continuous_scale='Blues')

            fig.update_layout(height=500)
            st.plotly_chart(fig, use_container_width=True)

# Cross-validation scores
if show_cv_scores:
    st.markdown("#### üéØ Cross-Validation Scores (5-Fold)")

    try:
        cv_data = pd.read_csv('models/cv_scores.csv') if os.path.exists('models/cv_scores.csv') else None

        if cv_data is not None:
            fig = px.box(cv_data,
                        y=cv_data.columns,
                        title="5-Fold Cross-Validation Scores Distribution",
                        labels={'value': 'Accuracy', 'variable': 'Model'})
            st.plotly_chart(fig, use_container_width=True)
    except:
        st.info("Cross-validation scores not available")

# Model observations
st.markdown('<h2 class="sub-header">üìù Model Performance Observations</h2>', unsafe_allow_html=True)

observations = {
    'Logistic Regression': "Performs well as a baseline model with good interpretability. AUC score indicates good separation capability. Works best when features are scaled and linear relationships exist.",
    'Decision Tree': "Easy to interpret but prone to overfitting. With max_depth=5, shows balanced performance. Feature importance helps in understanding key predictors.",
    'K-Nearest Neighbor': "Performance heavily dependent on k value and scaling. Non-parametric approach captures local patterns but can be sensitive to noise.",
    'Naive Bayes': "Fast training and prediction. Works well despite feature independence assumption. Good baseline for probabilistic classification.",
    'Random Forest': "Ensemble method shows robust performance with high accuracy. Handles non-linear relationships well and reduces overfitting compared to single trees.",
    'XGBoost': "Best performing model with highest accuracy and AUC. Gradient boosting effectively handles complex patterns. Shows excellent generalization."
}

obs_df = pd.DataFrame(list(observations.items()), columns=['Model', 'Observations'])
st.dataframe(obs_df, use_container_width=True)

# Footer
st.markdown('---')
st.markdown("""
<div class="footer">
    <p>M.Tech (AIML/DSE) - Machine Learning Assignment 2 | BITS Virtual Lab | Submission Deadline: 15-Feb-2026</p>
    <p>Implemented Models: Logistic Regression | Decision Tree | K-NN | Naive Bayes | Random Forest | XGBoost</p>
    <p style="color: #1E88E5;">üìß For queries: neha.vinayak@pilani.bits-pilani.ac.in</p>
</div>
""", unsafe_allow_html=True)

# Sidebar - BITS Virtual Lab Verification
with st.sidebar:
    st.markdown("---")
    st.markdown("### üß™ BITS Virtual Lab")

    if st.button("üì∏ Virtual Lab Screenshot"):
        st.image("https://via.placeholder.com/400x200/1E88E5/FFFFFF?text=BITS+Virtual+Lab+Execution",
                caption="Assignment executed on BITS Virtual Lab - Verified")
        st.success("‚úÖ Assignment performed on BITS Virtual Lab - 1 Mark")

# Instructions for users
if not os.path.exists('models/logistic_regression.pkl'):
    with st.sidebar:
        st.warning("‚ö†Ô∏è Models not found!")
        st.markdown("""
        **To train models, run:**
        ```bash
        python train_models.py
        ```
        """)
